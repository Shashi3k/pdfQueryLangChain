{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuil+PBEAgzpwhiaBDyFo4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shashi3k/pdfQueryLangChain/blob/main/PdfQueryLangchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpCvZ8fyVk3e",
        "outputId": "fbc4829e-ba48-4ecb-e448-1f5445a36682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.10-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.2/806.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.25 (from langchain)\n",
            "  Downloading langchain_community-0.0.25-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.28 (from langchain)\n",
            "  Downloading langchain_core-0.1.28-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.4/252.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.0 (from langchain)\n",
            "  Downloading langsmith-0.1.13-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.28->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.28->langchain) (23.2)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.28->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.28->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: orjson, mypy-extensions, marshmallow, jsonpointer, typing-inspect, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.10 langchain-community-0.0.25 langchain-core-0.1.28 langchain-text-splitters-0.0.1 langsmith-0.1.13 marshmallow-3.21.0 mypy-extensions-1.0.0 orjson-3.9.15 typing-inspect-0.9.0\n",
            "Collecting openai\n",
            "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.13.3\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "!pip install faiss-cpu\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "v09Zk9_UV6SN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = ''"
      ],
      "metadata": {
        "id": "gZaMn8axWkv1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdfreader = PdfReader('NNDL_Unit1 _updated.pdf')"
      ],
      "metadata": {
        "id": "vekaeZJBXM3K"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Concatenate\n",
        "raw_text = \"\"\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "  content = page.extract_text()\n",
        "  if content:\n",
        "    raw_text +=content"
      ],
      "metadata": {
        "id": "4_bVe-WXYSwn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "DxWhykcLYveJ",
        "outputId": "b0610dfb-0997-4258-d1e4-5f20659bc33d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Neural Network and Deep \\nLearningNeural Network and Deep Learning\\nCourse Content/Syllabus\\nUnit 1: Deep Learning Framework, TensorFlow and Keras\\nIntroduction to Deep Learning, Feed Forward Neural Network, Difference between FFNN \\nand DNN, Activation Functions and Loss functions, Need for -Batch normalization, \\nRegularization and Optimization. TensorFlow: Installation, Creating and Managing \\nGraphs, Lifecycle of a Node Value, Linear Regression, Gradient Descent, Visualizing \\nGraphs using TensorBoard. Keras: Installation, Loading Data, Defining and Compiling \\nModels, Fitting and Evaluating Models, Simple Neural Networks’ Implementation, Fine -\\nTuning Hyper parameters. Neural Network and Deep Learning\\nCourse Content/Syllabus\\nUnit 2: Convolutional Neural Network(CNN), Transfer Learning\\nArchitecture of CNNs, Filters, FeatureMaps, Max -Pool Layers, Other Pooling Types, \\nBack Propogation, Convolution Architectures -Alexnet, ZFNet, VGGNet, GoogleNet, \\nResNet, Case Study: Image Recognition UsingCNN –Hands -On Implementation Using \\nKeras. RCNN, FRCNN, Faster RCNN, YOLO V3, Transfer Learning -Motivation, \\nVariations. \\nUnit 3: Recurrent Neural Networks (RNN)\\nIntroduction -Recurrent Neurons, Memory Cells, Variable -Length Input -Output Sequences, \\nRNN Architecture, Sequence learning problem, BPTT -Back Propagation Through Time, \\ntruncated BPTT, Vanishing and Exploding Gradient, Bidirectional RNN, LSTM Cell and \\nGRU Cell, Text Classification with RNN, Encoder/Decoder architecture, Seq2Seq model \\nwith Attention Transformer model and BERT model , Transformer Attention and its \\nimplementation. Course Content/Syllabus\\nUnit 4: GAN, Unsupervised Feature Learning, GNN AND META \\nLEARNING\\nGAN -Architecture and Training Methods, Image -Generation, Hands -On \\nImplementation Using Keras, DCGAN with keras, Unsupervised Feature \\nLearning –Autoencoders , Regularization in autoencoders, Denoising \\nautoencoders, Sparse autoencoders, Contractive autoencoders and Variational \\nAuto Encoders, GNN –Graphical Neural Networks, Graph Convolutional \\nNetworks, Applications of GNN, Introduction to Meta Learning -Siamese \\nNetwork.Neural Network and Deep Learning\\nCourse Objectives\\n•Introduce students to Deep Neural Network and programming with Tensor Flow \\nand Keras tools.\\n•Introduce students to Deep Learning(CNN, RNN) and Transfer \\nLearning Techniques.\\n•Introduce students to Generative Adversarial Networks and Graph \\nNeural Network.\\n•Introduce students to Deep Reinforcement Learning.Neural Network and Deep Learning\\nCourse Outcomes\\nAttheendofthiscourse, the student willbe able to:\\n•Implement aNeural Network using Tensor Flow andKeras\\n•Classify images using CNN.\\n•Solve time-series related problems with RNN.\\n•Generate data intheform ofimages using GAN.\\n•Develop asimple game engine using Reinforcement Learning.Neural Network and Deep Learning\\nTools/Text Book & Reference Books\\nTools / Languages : Tensorflow 1.15, Keras 2.3.1, Python 3.7.\\nText Book:\\n1.“Advanced Deep Learning with Python” -Ivan Vasilev , Packt Publishing, \\n2019\\n2.Neural Network and Deep learning by Charu C Agarwal, Springer\\n.\\nReference Books:\\n1: “Hands -on Machine Learning with Scikit -Learn and TensorFlow”, Aurelian \\nGeron, O’REILLY , 1st\\nEdition, 2017.\\n2: “Deep Learning with Keras”, Antonio Gulli and Sujit Pal, Packt Publishing, \\n1st Edition, 2017.  3: “Pattern Recognition and Machine Learning”, Christopher \\nBishop, Springer, 1st Edition, 2011  (Reprint).\\n4: Handouts for SVM, Transfer Learning.Neural Network and Deep Learning\\nOverview\\nUnit 1: Deep Learning Framework, TensorFlow and Keras\\nIntroduction to Deep Learning, Feed Forward Neural Network, Difference between  \\nFFNN and DNN, Activation Functions and Loss functions, Need for -Batch normalization,  \\nRegularization and Optimization.\\nTensorFlow : Installation, Creating and Managing Graphs, Lifecycle of a Node Value,  \\nLinear Regression, Gradient Descent, Visualizing Graphs using TensorBoard.\\nKeras : Installation, Loading Data, Defining and Compiling Models, Fitting and Evaluating\\nModels, Simple Neural Networks’ Implementation, Fine -Tuning Hyper parameters.TOPICS INDEEP LEARNING\\nIntroduction &\\nBrief Overview ofDeep LearningTOPICS IN DEEP LEARNING  \\nHUMAN INTELLIGENCE\\nTOPICS INDEEP LEARNING\\nPrerequiste\\nInterest in Deep Learning and AI\\nOne programming language\\nCommitmnet toPersevere\\nLittle knowledge in Machine IntelligenceTOPICS IN DEEP LEARNING\\nMotivation\\n•It is redefining the way computing is being used to \\nsolve the societal problems\\n•It is one of the fastest growing technology being \\nadapted across various disciplines\\n•Most of the established companies and startups now \\nhave an element of AI to build new application.TOPICS IN DEEP LEARNING\\nMotivation\\n•It is redefining the way computing is being used to \\nsolve the societal problems\\n•It is one of the fastest growing technology being \\nadapted across various disciplines\\n•Most of the established companies and startups now \\nhave an element of AI to build new application.TOPICS INDEEP LEARNING\\nAIV/SMLV/SDL\\nTOPICS INDEEP LEARNING\\nAIV/SMLV/SDL\\nTOPICS IN DEEP LEARNING  \\nARTIFICIAL INTELLIGENCE\\nArtificial intelligence as an academic discipline was founded in 1950s.\\nActually the “AI” term was coined by John McCarthy , an American \\ncomputer scientist, back in 1956 at The Dartmouth Conference.\\nAccording to John McCarthy, AI is “The science and engineering of \\nmaking intelligent machines, especially intelligent computer programs”.\\nThough it was not until recently it became part of daily life thanks \\ntoadvances in big data availability and affordable high computing \\npower. AI works at its best by combining large amounts of data sets with \\nfast, iterative processing and intelligent algorithms.\\nThis allows the AI software to learn automatically from patterns or features \\ninthat vast data sets.TOPICS IN DEEP LEARNING  AI EVOLUTION\\nTOPICS IN DEEP LEARNING\\nKind of Applications where ML/DL used\\n1.Problems where there is no deterministic algorithm exists. Example : \\nRecognizing a 3D object from a given scene, Hand writing recognition, \\nspeech recognition\\n2.Problems which do not have any fixed solution, because goals keep \\nchanging. System adapts and learns from experience e.g. SPAM \\nemails, Financial Fraud, IT security framework\\n3.Where solutions are individual specific or time dependent, \\nexample recommendations and targeted advertisements.\\n4.Prediction of share pricesTOPICS INDEEP LEARNING\\n1.Machine Learning vs.Deep Learning: Decision Boundary\\nInthe case ofclassification problems, the\\nalgorithm learns the function that separates 2\\nclasses –this isknown asaDecision boundary .A\\ndecision boundary helps us indetermining\\nwhether agiven data point belongs toapositive\\nclass oranegative class .\\nEvery Machine Learning algorithm isnot capable\\noflearning allthe functions .This limits the\\nproblems these algorithms can solve that involve\\nacomplex relationship .TOPICS INDEEP LEARNING\\n2.Machine Learning vs.Deep Learning: Feature Engineerin g\\nExtracting features manually from animage needs strong\\nknowledge ofthe subject aswell asthe domain .Itisan\\nextremely time -consuming process .Thanks toDeep Learning,\\nwecanautomate theprocess ofFeature Engineering!TOPICS INDEEP LEARNING\\nWhy DEEP LEARNING ?\\nWhen the amount of data is increased, machine learning algorithms techniques are insufficient in terms of  \\nperformance and deep learning gives better performance like accuracy\\nWhy now?\\n1.Advanced Algorithms,  APIs,Platfoms\\n2.GPU Computing\\n3.Availability of large training data(  due to smart phones and  devices) in millionsTOPICS INDEEP LEARNING\\nWhy GPU ?\\nTOPICS INDEEP LEARNING\\nDeeplearning/AI Applications\\n•Precision Agriculture\\n•Learner Profiling\\n•Video Captioning\\n•Exploring patterns and\\nImages\\n•Image detection in health  \\ncare\\n•Identifying specific markers\\nin Genomes\\n•Recommendations\\n•Behavior prediction\\n•etcTOPICS IN DEEP LEARNING  \\nDEEP LEARNING\\n•Deep learning structures algorithms in layers to create an \"artificial neural network”\\nthat can learn and make intelligent decisions on its own\\n•In practical terms, deep learning is just a subset of machine learning. In fact, deep  \\nlearning technically is machine learning and functions in a similar way (hence why the  \\nterms are sometimes loosely interchanged). However, its capabilities are different.\\n•While basic machine learning models do become progressively better at whatever their\\nfunction is, they still need some guidance.\\n•If an AI algorithm returns an inaccurate prediction, then an engineer has to step in and  \\nmake adjustments.\\n•With a deep learning model, an algorithm can determine on its own if a prediction is\\naccurate or not through its own neural network.TOPICS INDEEP LEARNING\\nThree main areas where Deep Learning isbeing prominently applied1.Detection•Text & Speech\\n•Image  interpretation\\n•Human behaviour\\n& Identity\\n•Abuse & Fraud\\n2.Prediction•Recommendations\\n•Individual  behaviour &  \\ncondition\\n•Collective  behaviour\\n3.Generation•Visual Art\\n•Music\\n•T ext\\n•DesignTOPICS INDEEP LEARNING\\nTypes ofLearning algorithms\\n1.Supe rvised\\n•Training data islabeled\\n•Goal iscorrectly label new data\\n2.Unsupervised\\n•Training data isunlabeled\\n•Goal isto categorize theobservations\\n3.Reinforcement\\n•Training data isunlabeled\\n•System receives feedback foritsactions\\n•Goal istoperform better actionsTOPICS IN DEEP LEARNING\\nNeural networks and Deep learning\\nDeep Learning isthebranch ofMachine Learning based onDeep Neural\\nNetworks (DNNs, i.e.,neural networks composed ofmore than 1hidden\\nlayer) .TOPICS INDEEP LEARNING\\nNetworks ofNeurons\\nTOPICS INDEEP LEARNING\\nNetworks ofNeurons\\nTOPICS INDEEP LEARNING\\nBasic tounderstand Neural Network -Relation toBiological Neuron\\nFunctionality ofNeuron\\ndendrite :receives signals from other neurons\\nsynapse: point ofconnection toother neurons\\nsoma: processes theinformation\\naxon: transmits theoutput of thisneuronTOPICS IN DEEP LEARNING\\nBasic to understand Neural Network -Relation to Biological Neuron\\nBiological Neuron Working\\n•Input aswell asoutput isknown .\\n•From this input togetthedesired output only few neurons must\\nbeactivated .I will know my Input as well as output…I will try to create the  \\nnetwork/model that will take my input and give me the output  \\nas expected by me\\nThat’s all is Neural Networks in Nutshell“\\nTOPICS INDEEP LEARNING\\nNeural NetworkDataset Contains enough information to successfully create a deep  \\nlearning model\\nHow brain learns same way make the modelled neural  \\nnetwork to learn the data\\nWhether the network has learned properly or not is verified  \\nusing Test Data\\nDepends on the type of output requiredTraining Data\\nTesting Data\\nMathematicsTOPICS INDEEP LEARNING\\nToConstruct this network what allisrequired tome?Input and OutputHow to model  \\nNeural Network  \\nMathematicallyFreeze the model  \\ndepending on the  \\ntype of problem  \\nstatementTrain the model  \\nusing the training  \\nDataTest the modelled  \\nnetwork with test  \\ndataTOPICS INDEEP LEARNING\\nMathematical importanceNeural Network and Deep Learning\\nNeural Networks Overview\\nNeural Network and Deep Learning\\nFeed forward Neural Network/ANN\\n•The feed forward model is a basic type of neural network because the input is only \\nprocessed in one direction. Feed forward neural networks are artificial neural networks in \\nwhich nodes do not form loops . This type of neural network is also known as a multi -layer \\nneural network as all information is only passed forward.\\n•During data flow, input nodes receive data, which travel through hidden layers, and exit \\noutput nodes.\\n•As a feed forward neural network model, the single -layer perceptron often gets used for \\nclassification. Machine learning can also get integrated into single -layer perceptrons. \\nThrough training, neural networks can adjust their weights based on a property called the \\ndelta rule , which helps them compare their outputs with the intended values.\\n•As a result of training and learning, gradient descent occurs. Similarly, multi -layered \\nperceptrons update their weights. But, this process gets known as back -propagation . If \\nthis is the case, the network\\'s hidden layers will get adjusted according to the output values \\nproduced by the final layer. Backpropagation is a technique based on gradient descentNeural Network and Deep Learning\\nFeed forward Neural Network/ANN\\nANN canbeused tosolve problems related to:\\n•Tabular data\\n•Imagedata\\n•Text dataNeural Network and Deep Learning\\nChallenges with Neural Network\\n•While solving an image classification problem using ANN, the first step is to \\nconvert a 2 -dimensional image into a 1 -dimensional vector prior to training the \\nmodel. This has two drawbacks:\\n•The number of trainable parameters increases drastically with an increase in the \\nsize of the image. If the size of the image is 224*224, then the number of \\ntrainable parameters at the first hidden layer is 602,112. That’s huge!\\n•ANN loses the spatial features of an image. Spatial features refer to the \\narrangement of the pixels in an image.\\n•ANN cannot capture sequential information in the input data which is required \\nfor dealing with sequence data\\n•to overcome the limitations of MLP using two different architectures –\\nRecurrent Neural\\nNetworks (RNN) and Convolution Neural Networks (CNN).\\nNeural Network and Deep Learning\\nConvolution Neural Network\\nThe building blocks ofCNNs arefilters\\na.k.a.kernels .Kernels areused toextract the\\nrelevant features from theinput using the\\nconvolution operation .\\nCNN learns the filters automatically without \\nmentioning it explicitly. These filters help in \\nextracting the right and relevant features from \\nthe input data\\nCNN captures the spatial features from \\nanimage. Spatial features refer to \\nthearrangement of pixels and the \\nrelationship between them in an image. They \\nhelp us in identifying the object accurately, \\nthelocation of an object, as well as its \\nrelation with other objects in an imageNeural Network and Deep Learning\\nRecurrent Neural Network\\n•Artificial neural networks (ANN) are feedforward networks that \\ntake inputs and produce outputs, whereas RNNs learn from \\nprevious outputs to provide better results the following time.\\n•Apple\\'s Siri and Google\\'s voice search algorithm are \\nexemplary applications of RNNs in machine learning.\\n•As the name implies, recurrent neural networks have a \\nrecurrent connection in which the output is transmitted back \\nto the RNN neuron rather than only passing it to the next \\nnode .\\n•Each node in the RNN model functions as a memory cell, \\ncontinuing calculation and operation implementation.\\n•An RNN remembers every piece of information throughout \\ntime. It is only effective in time series prediction because of the \\nability to recall past inputs. This is referred to as long short -term \\nmemory (LSTM)\\nNeural Network and Deep \\nLearning 1ANN Vs RNN Vs CNN\\nNeural Network and Deep Learning\\n1ANN Vs RNN Vs CNN\\nTOPICS INDEEP LEARNING\\nFeed Forward Neural NetworkNeural Network and Deep Learning\\nNeural Networks Overview\\nNeural Network and Deep Learning\\nNeural Network\\n•Perceptron isasingle layer\\nneural network(basic unit ofNN).\\n•Aperceptron takes avector of\\nreal-valued inputs, calculates a\\nlinear combination of these\\ninputs, then outputs a1ifthe\\nresult isgreater than some\\nthreshold and -1otherwise\\n•Given inputs x1through xn,the\\noutput O(x1,...,xn)computed\\nbytheperceptron is\\n.\\nEvery neuron two functions areassociated:\\nPreactivation Function\\nActivation FunctionNeural Network and Deep Learning\\nAPerceptron\\nNeural Network and Deep Learning\\nPerceptron\\nLearning a perceptron involves choosing values for the  \\nweights w0 , . . . , wn .Therefore, the space H of candidate  \\nhypotheses considered in perceptron learning is the set of  \\nall possible real -valued weight vectors\\nWhy do we need Weights and Bias?\\nWeights shows the strength of the particular node.\\nA bias value allows you to shift the activation function \\ncurve  up or down\\n•Perceptron rule\\n•Delta rule\\nThese two algorithms are guaranteed to converge to somewhat different  \\nacceptable hypotheses, under somewhat different conditions. They are important  \\nto ANNs because they provide the basis for learning networks of many units.Neural Network and Deep Learning\\nHow tolearn weights forasingle perceptron?Perceptron Training Rule\\nThe learning problem is to determine a weight vector that causes  the \\nperceptron to produce the correct + 1 or -1 output for each  of the \\ngiven training examples.\\nTo learn an acceptable weight vector\\n1.Begin with random weights, then iteratively apply the perceptron to each  training example, \\nmodifying the perceptron weights whenever it misclassifies  an example.\\n2.This process is repeated, iterating through the training examples as many\\ntimes as needed until the perceptron classifies all training examples correctly.\\n3.Weights are modified at each step according to the perceptron training rule,\\nwhich revises the weight wi associated with input xi according to the rule.Neural Network and Deep LearningPerceptron Training Rule\\n•The role ofthelearning rate istomoderate\\nthedegree towhich weights arechanged at\\neach step .Itisusually settosome small\\nvalue (e.g.,0.1)and issometimes made to\\ndecay asthe number ofweight -tuning\\niterations increasesNeural Network and Deep LearningDisadvantage ofperceptron rule\\n•Perceptron rule finds a successful weight vector and converges  \\nonly when the training examples are linearly separable, It fails to  \\nconverge if the examples are not linearly separable.\\n•An iterative algorithm is said to converge when as the iterations  \\nproceed the output gets closer and closer to a specific value.\\n•If the data are not linearly separable, convergence is not assured.Neural Network and Deep LearningNeural Network and Deep Learning\\nDelta Rule\\n•The delta rule converges towards a best -fit approximation to the target concept.\\n•Delta learning does this using the difference between a target activation and an actual\\nobtained activation.\\n•Another way to explain the Delta rule is that it uses an error function to perform  \\ngradient descent learning.\\n•The actual implementation of the Delta rule is going to vary according to the network  \\nand its composition, but by employing a linear activation function, the Delta rule can  \\nbe useful in refining some types of neural network systems with particular flavors of  \\nbackpropagation.Gradient Descent and Delta Rule\\n•Ifthetraining examples arenot linearly separable, thedelta\\nrule converges toward abest -fitapproximation tothetarget\\nconcept .\\n•The keyidea behind thedelta rule istousegradient descent\\ntosearch thehypothesis space ofpossible weight vectors to\\nfind theweights that best fitthetraining examples .\\n•Tounderstand the delta training rule, consider the task of\\ntraining anun-thresholded perceptron .That is,alinear unit\\nforwhich theoutput Oisgiven by\\nNeural Network and Deep LearningGradient Descent and Delta Rule\\n•Toderive aweight learning rule forlinear units, specify a\\nmeasure for the training error ofahypothesis (weight\\nvector), relative tothetraining examples\\nWhere,\\n•D is the set of training examples, t d is the target output for  \\ntraining example d, o d is the output of the linear unit for  \\ntraining example d\\n•E [ w ] is simply half the squared difference between the  \\ntarget output t d and the linear unit output o d, summed  over \\nall training examples.LOSS FUNCTIONNeural Network and Deep LearningNeural Network and Deep Learning\\nGradient Descent\\n•Gradient Descent searches the hypothesis space for possible weight vectors to find the  \\nmost optimal pair of weights for the training example.\\n•In any training scenario, we will have a loss function to judge how well the weights  have \\nfit for the model.\\n•What gradient descent does essentially is to take a small step in a direction and try to\\nreach the minimal point it can in that direction as soon as possible.\\n•Choosing this direction needs to be the direction of the steepest descent. That can be  \\nfound using the derivative of our loss function.\\n•Lets take the example of loss function to be 𝐸1=𝛴𝑡𝑑− 𝑜𝑑\\n22.\\n•𝑡isthetarget output and oistheactual output.Neural Network and Deep Learning\\nGradient Descent\\nLets take the example of loss function to be 𝐸\\nഥ\\nand o is the actual output.1=𝛴𝑡𝑑− 𝑜𝑑2. 𝑡is the target output\\n2\\nLets take the derivative ofthis to tryand find thesteepest descent.\\n∇𝐸𝑣ҧ\\n=𝜕𝐸𝜕𝐸, , ,……,𝜕𝐸 𝜕𝐸\\n𝜕𝑤1𝜕𝑤2𝜕𝑤3𝜕𝑤𝑛\\nThis is because there are many weights that need to be modified. The gradient ∇𝐸𝑣ҧ\\n,                                                                                                                            gives \\nthe direction of the steepest ascent, therefore the direction of steepest descent is  \\ngiven by -∇𝐸𝑣ҧ .Neural Network and Deep Learning\\nGradient Descent\\nNow we need to find out how to alter the weights to move in the direction of the steepest\\ndecreases. That can be said to calculated in the following manner.\\nW = 𝑤ഥ = 𝑤ഥ+ Δ𝑤Δ𝑤= −𝜂∇𝐸𝑤ഥ\\nThe negative sign is to indicate that it is steepest descent that is being found.\\nWe know that 𝐸\\n𝑤ഥ1=𝛴𝑡𝑑− 𝑜𝑑\\n22.\\nTherefore we cansaythat,\\n𝑖𝜕𝐸\\nΔ𝑤 = −𝜂\\n= −𝜂𝜕𝑤𝑖𝜕𝑤𝑖2𝜕 1\\n𝛴𝑡𝑑− 𝑜𝑑2= −𝜂\\u0dce𝜕\\n𝜕𝑤𝑖𝑑𝑡− 𝑂𝑑\\nNow by delta rule we know that 𝑂𝑑=𝑤𝑑∗ 𝑥𝑑\\nTherefore we can say that\\nΔ𝑤𝑖= 𝜂*𝑡𝑑− 𝑂𝑑∗ (𝑥𝑑)Neural Network and Deep Learning\\nGradient Descent\\nIn any case, the gradient descent weight change will ensure  \\nthat we are heading towards the minima.\\n𝑤𝑖= 𝑤𝑖− 𝜂∇𝐸𝑤\\nIn the case that the gradient is negative, the weight will be  \\nupdated to increase and thus move to the right as shown \\nin  case 1. \\nIn the case that the gradient is positive, the weight will be  \\nupdated to decrease and thus move to the left as shown in  \\ncase 2.\\nCase 1Case 2Neural Network and Deep Learning\\nLearning rate ingradient descent\\n𝜂iscalled thelearning rate. This isused todetermine how much theweights should change by.This isavery\\nimportant hyper -parameter. This value must bechosen properly.\\nNeural Network and Deep Learning\\nTraining aNeural Network\\n1.Randomly initialize weights (𝜣or w)\\n2.Implement propagation toget h𝜣(xi)foranyinstance xi.\\n3.Implement code tocompute cost function J(𝜣).\\n4.Implement backprop tocompute partial derivatives𝜕𝐽(𝜃)\\n𝜕𝜃𝑗𝑘\\n𝑖\\n5.Use gradient descent with backprop tofitthenetworkTheBACKPROPAGATIONAlgorithm\\n•The BACKPROPAGATION Algorithm learns theweights foramultilayer\\nnetwork, given anetwork with afixed set of units and\\ninterconnections .\\n•Itemploys gradient descent toattempt tominimize thesquared error\\nbetween the network output values and thetarget values forthese\\noutputs .\\n•InBACKPROPAGATION algorithm, weconsider networks with multiple\\noutput units rather than single units asbefore, sowe redefine Eto\\nsum theerrors over allofthenetwork output units .\\nNeural Network and Deep LearningNeural Network and Deep Learning\\nGradient Descent -Regression Model( basic)\\nNeural Network and Deep Learning\\nDerivatives inLogistic Regression( basic)\\nNeural Network and Deep Learning\\nComputing Gradients\\nNeural Network and Deep Learning\\nBackward Propagation -Intuition\\nNeural Network and Deep Learning\\nBackward Propagation -Intuition\\nNeural Network and Deep Learning\\nBackward Propagation -Intuition\\nNeural Network and Deep Learning\\nBackward Propagation -Issues\\nTOPICS INDEEP LEARNING\\nActivation Functions and Loss  \\nFunctionsNeural Network and Deep Learning\\nActivation Functions\\n1.An activation function is a function that is added to an artificial neural \\nnetwork in order to help the  network learn complex patterns in the data .\\n2.An activation function is a function used in artificial neural networks which \\noutputs a small value for small inputs, and a larger value if its inputs exceed a \\nthreshold. \\n3.If the inputs are large enough, the  activation function \"fires\", otherwise it \\ndoes nothing. In other words, an activation function is like a  gate that checks \\nthat an incoming value is greater than a critical number. \\n4.Activation functions are  useful because they add non -linearities into neural \\nnetworks, allowing the neural networks to learn  powerful operations. \\n5.If the activation functions were to be removed from a feedforward neural  \\nnetwork, the entire network could be re -factored to a simple linear operation \\nor matrix  transformation on its input, and it would no longer be capable of \\nperforming complex tasks such as image recognition.Neural Network and Deep Learning\\nTypes ofActivation functions\\nA.Binary step function\\nB.Linear function\\nC.Non linear activation function\\ni)Sigmoid\\nii)Softmax\\niii)ReLU\\niv)Leaky ReLU\\nv)tanh\\nNeural Network and Deep Learning\\nActivation Functions\\nTypically the type of problems we consider under a  machine learning \\nenvironment falls either as  classification or regression problem.\\nSo we need to know\\n1.What should be the activation function for a  regression problem?\\n2.What should be the activation function for classification problem?Neural Network and Deep Learning\\nWhat is a Step Function?\\nBinary step function depends on a threshold value that decides  \\nwhether a neuron should be activated or not.\\nThe input fed to the activation function is compared to a certain  \\nthreshold; if the input is greater than it, then the neuron is \\nactivated,  else it is deactivated, meaning that its output is not \\npassed on to the  next hidden layer.\\nInput:\\nweighted sum of the inputs and biases of the neurons in a layer\\nOutput:\\nA value of 0 or 1\\nDisadvantage:\\n•It cannot provide multi -value outputs —for example, it cannot \\nbe  used for multi -class classification problems.\\n•The gradient of the step function is zero, which causes a \\nhindrance in the backpropagation process.\\nNeural Network and Deep Learning\\nWhat is a Linear Activation Function?\\nThe linear activation function, also known as\"no\\nactivation,\" or\"identity function\" (multiplied x1.0),is\\nwhere theactivation isproportional totheinput .\\nThefunction doesn\\'t doanything totheweighted sum ofthe\\ninput, itsimply spits outthevalue itwasgiven .\\nInput:\\nweighted sum of the inputs and biases of the neurons in a \\nlayer\\nOutput:\\nit allows multiple outputs, not just yes and no.\\nDisadvantage:\\n•It’s not possible to use backpropagation as the derivative of \\nthe function is a constant and has no relation to the input x.\\n•All layers of the neural network will collapse into one if a \\nlinear activation  function is used. No matter the number of \\nlayers in the neural network,  the last layer will still be a \\nlinear function of the first layer. So, essentially,  a linear \\nactivation function turns the neural network into just one \\nlayer.\\nNeural Network and Deep Learning\\nNon Linear Activation Function\\nThe linear activation function shown above is simply a linear regression model.  Because of its limited power, \\nthis does not allow the model to create complex mappings between the network ’s inputs and outputs.\\nNon-linear activation functions solve the following limitations oflinear activation\\nfunctions :\\n•They allow backpropagation because now thederivative function would berelated tothe\\ninput, andit’spossible togoback andunderstand which weights intheinput neurons can\\nprovide abetter prediction .\\n•They allow the stacking of multiple layers of neurons as the output would now be a  non -linear \\ncombination of input passed through multiple layers. Any output can be  represented as a \\nfunctional computation in a neural network.Neural Network and Deep Learning\\n1.Sigmoid Function or Logistic Function\\nInput: weighted sum of the inputs and biases of the neurons in a layer\\nOutput: A value between 0 and 1\\nVariation: Continuously differentiable over different values of z and has  a fixed \\noutput range.\\nAdvantage: Binary classification problems\\ncapture the probability  ranging from 0 to 1 .\\nDisadvantages:\\n•value of f(x) increases at a very slow rate\\n•susceptible to the vanishing gradient problem\\n•becomes hard to optimize\\nVanishing gradient —for very high or very low values of z, there is  almost no \\nchange to the prediction, causing a vanishing gradient  problem. This can result \\nin the network refusing to learn further, or  being too slow to reach an accurate \\nprediction.\\nOutputs not zero centered .\\nComputationally expensive\\nNeural Network and Deep Learning\\n1.Sigmoid Function or Logistic Function\\nHere’s why sigmoid/logistic activation function is one of \\nthe most widely used functions:\\nIt is commonly used for models where we have to predict \\nthe  probability as an output. Since probability of \\nanything exists only  between the range of 0 and 1, \\nsigmoid is the right choice because of  its range.\\n•The function is differentiable and provides a smooth \\ngradient, i.e.,  preventing jumps in output values. This is \\nrepresented by an S -shape of the sigmoid activation \\nfunction.\\nthe derivative of the sigmoid function is\\nNeural Network and Deep Learning\\n1.Sigmoid Function or Logistic Function\\n•As we can see from the above Figure, the gradient  values are \\nonly significant for range -3 to 3, and the  graph gets much \\nflatter in other regions.\\n•It implies that for values greater than 3 or less than -3,  the \\nfunction will have very small gradients. As the  gradient value \\napproaches zero, the network ceases to  learn and suffers from \\nthe Vanishing gradient problem.\\n•The output of the logistic function is not symmetric  around \\nzero. So the output of all the neurons will be of the  same sign. \\nThis makes the training of the neural\\nnetwork more difficult and unstable.\\nNeural Network and Deep Learning\\nVanishing Gradient\\nFor the nodes with sigmoid activation functions, we know that the \\npartial  derivative of the sigmoid function reaches a maximum \\nvalue of 0.25 .\\nWhen there are more layers in the network, the value of the \\nproduct of  derivative decreases until at some point the partial \\nderivative of the loss  function approaches a value close to zero, \\nand the partial derivative  vanishes. We call this the vanishing \\ngradient problem.\\nWith shallow networks, sigmoid function can be used as the small \\nvalue of  gradient does not become an issue.\\nWhen it comes to deep networks, the vanishing gradient could \\nhave a  significant impact on performance. The weights of the \\nnetwork remain  unchanged as the derivative vanishes.\\nDuring back propagation, a neural network learns by updating its \\nweights  and biases to reduce the loss function. In a network with \\nvanishing  gradient, the weights cannot be updated, so the network \\ncannot learn.\\nThe performance of the network will decrease as a result.\\nNeural Network and Deep Learning\\nWhat isaTanh Function?\\nInput:\\nweighted sum of the inputs and biases of the neurons in a layer\\nOutput:\\nBounded between -1 to 1.\\nVariation:\\ngradient or the derivative of the\\nTanh function is steeper as compared to the \\nsigmoid function . It is the shifted version of sigmoid with \\nmean value of zero\\nAdvantage:\\nZero centered —making it easier to model inputs that have \\nstrongly  negative, neutral, and strongly positive values.\\nOtherwise like the Sigmoid function.\\nDisadvantages:•susceptible to the vanishing gradient problem\\n•Model slows down exponentially beyond +2 and -2.https://towardsdatascience.com/activation -functions -in-neural -networks -\\n83ff7f46a6bd\\nNeural Network and Deep Learning\\nWhat is a ReLU Function? Rectifying Linear Unit\\nInput:\\nweighted sum of the inputs and biases of the neurons \\nin a layer\\nOutput:\\nThe function simply outputs the value of\\n0 if it receives any  negative input, but for any \\npositive value z, it returns that value back like a linear \\nfunction\\nf(z)=max( 0,z)\\nAdvantage:•Solves gradient descent problem\\n•Computationally economical\\n•Network is Sparse•Disadvantages:\\n•Dying Relu Problem -—when inputs approach zero, orare\\nnegative, the gradient of the function becomes zero, the  network \\ncannot perform backpropagation and cannot learn.\\nItisthedefault function used now.Neural Network and Deep Learning\\nWhat isaLeaky ReLU Function?\\nInput:\\nweighted sum of the inputs and biases of the neurons in a layer\\nOutput:\\nThe function simply outputs the value of 0 if it \\nreceives any  negative input, but for any positive value z, it \\nreturns that value\\nback like a linear function\\nf(z)=max(0.1z,z)\\nVariation:\\nwhere slope is changed left of x=0 as shown in figure and thus  \\ncausing a leak and extending the range of ReLU.\\nAdvantage:\\nPrevents dying ReLU problem —this variation of ReLU has a small \\npositive slope in the negative area, so it does enable backpropagation, \\neven for  negative input values\\nIntroduce asmall slope tokeep theupdate aliveNeural Network and Deep Learning\\nWhat is a Softmax Function?\\nSoftmax is a very interesting activation function because \\nit not  only maps our output to a [ 0,1] range but also \\nmaps each  output in such a way that the total sum is 1. \\nThe output of  Softmax is therefore a probability \\ndistribution .\\nMathematically Softmax isthefollowing function where\\nzisvector ofinputs tooutput layer and jindexes the\\noutput units from 1,2,3….k:\\nSoftmax Function\\nIn conclusion, Softmax is used for multi -classification in logistic  regression model whereas \\nSigmoid is used for binary  classification in logistic regression model , the sum of  \\nprobabilities is One for Softmax.\\nNeural Network and Deep Learning\\nWhat isaSoftmax Function?\\nNeural Network and Deep Learning\\nUnderstanding Softmax Function\\nFind a(z)Neural Network and Deep Learning\\nUnderstanding Softmax Function\\nNeural Network and Deep Learning\\nActivations and their deriavativesNeural Network and Deep Learning\\nSwish: A Recent Development\\nA recent development where researchers have proposed alternative  activation functions to \\nspeed up large model training and hyperparameter  optimization tasks.\\nSwish is such a function, proposed by the famous Google Brain team ( in a paper where they \\nsearched for optimum activation function using complex  reinforcement learning techniques).\\nf(x) =x.sigmoid(x)Neural Network and Deep Learning\\nActivation Functions\\n•We have to choose activation functions for the hidden layer and the output layer. \\nEach layer can have its own activation function.\\n•The choice of activation function depends on the problem that we intend to \\nsolve.\\n• Usually, all hidden layers will have the same activation function and the \\nmost commonly used  functions are sigmoid and tanh functions.\\n•However, as we have seen, they have their shortcomings and\\nwe have seen others that are capable of replacing them.\\n•The output layer will also have its own activation  function based on the final \\noutput that is needed to solve the problem at hand.\\nNeural Network and Deep Learning\\n1Activation Functions\\nChoosing an activation \\nfunction  for the hidden \\nlayer mainly  depends on \\nwhat we expect the  hidden \\nlayer to do. There are  \\nmany different types of \\nneural  networks and \\ndepending on the  purpose \\nwe will be able to decide  \\nwhich particular activation  \\nfunction to utilize.\\nNeural Network and Deep Learning\\nWhat is a loss function?\\n•The function wewant tominimize ormaximize iscalled the objective function orcriterion .When weare\\nminimizing it,wemay also callitthecostfunction, lossfunction, orerror function .\\n•It is a function that tells as a quantifiable value of how much error does our approximated model possess .\\n•Typically, this involves the difference between the actual  value and approximated(predicted) value.\\n•Again based on the problem considered, the loss function also varies.Neural Network and Deep Learning\\nTypes of loss function\\nRegression Loss Functions  \\nMean Squared Error Loss\\nMean Squared Logarithmic Error Loss  \\nMean Absolute Error Loss\\nBinary Classification Loss Functions\\nBinary Cross -Entropy  \\nHinge Loss\\nSquared Hinge Loss\\nMulti -Class Classification Loss Functions  \\nMulti -Class Cross -Entropy Loss  \\nSparse Multiclass Cross -Entropy Loss  \\nKullback Leibler Divergence Loss\\nCross -entropy loss is often simply referred to as “ cross -entropy ,” “logarithmic loss ,” “logistic \\nloss,” or “ log loss ” for short.Neural Network and Deep Learning\\nRegression: Predicting a numerical value(Preferred Activation F: Linear orRELU )\\nLoss Function\\nThere are three metrics which are generally used for evaluation  \\nof Regression problems (like Linear Regression, Decision Tree\\nRegression, Random Forest Regression etc.):\\nMean Absolute Error (MAE) : This measures the absolute  \\naverage distance between the real data and the predicted data,\\nbut it fails to punish large errors in prediction.\\nMean Square Error (MSE) : This measures the squared \\naverage distance between the real data and the predicted data. \\nHere,  larger errors are well noted (better than MAE). But the  \\ndisadvantage is that it also squares up the units of data as well.\\nSo, evaluation with different units is not at all justified.\\nRoot Mean Squared Error (RMSE) : This is actually the \\nsquare root of MSE. Also, this metrics solves the problem of \\nsquaring the units.\\nThe final layer of the neural network will have one  \\nneuron and the value it returns is a continuous  \\nnumerical value.Neural Network and Deep Learning\\nCategorical: Predicting abinary outcome (Preferred Activation F :Sigmoid )\\nLoss Function\\nBinary Cross Entropy —Cross entropy quantifies the  difference between two probability distribution. Our  model \\npredicts a model distribution of {p, 1-p} as we  have a binary distribution. We use binary cross -entropy  to compare this \\nwith the true distribution {y, 1-y}\\nThe final layer of the neural network will have one neuron  and will return a value between 0 and 1, which can be  \\ninferred as a probably.Neural Network and Deep Learning\\nCategorical: Predicting a single label from multiple classes\\n(Preferred Activation F : Softmax )\\nLoss Function\\nCross Entropy —Cross entropy quantifies the  difference between two probability distribution. Our  model predicts a model \\ndistribution of {p1, p2, p3}  (where p1+p2+p3 = 1). We use cross -entropy to  compare this with the true distribution {y1, y2, y3 }\\nThe final layer oftheneural network willhave one neuron foreach oftheclasses and they willreturn avalue between 0and 1,\\nwhich canbeinferred asaprobably .The output then results inaprobability distribution asitsums to1.\\nNeural Network and Deep Learning\\nCategorical: Predicting multiple labels from multiple classes\\n(Preferred Activation F:Sigmoid )\\nLoss FunctionE.g. predicting thepresence of animals inanimage\\nBinary Cross Entropy —Cross entropy quantifies the  \\ndifference between two probability distribution . \\nOur model  predicts a model distribution of {p, 1-p} \\n(binary distribution)  for each of the classes. We use \\nbinary cross -entropy to  compare these with the \\ntrue distributions {y, 1-y} for each  class and sum up \\ntheir results\\nThe final layer ofthe neural network will have one\\nneuron for each oftheclasses and they willreturn a\\nvalue between 0and 1,which can beinferred asa\\nprobably .Neural Network and Deep Learning\\nWhich Loss Function to Use?\\nThe choice of cost function is tightly coupled with the choice of output unit. Most of the time, \\nwe  simply use the cross -entropy between the data distribution and the model distribution. The  \\nchoice of how to represent the output then determines the form of the cross -entropy function.\\n1.Regression Problem\\nA problem where you predict a real -value quantity.\\nOutput Layer Configuration : One node with a linear activation unit.\\nLoss Function : Mean Squared Error (MSE).\\n2.Binary Classification Problem\\nA problem where you classify an example as belonging to one of two classes.\\nThe problem is framed as predicting the likelihood of an example belonging to class  one, e.g. \\nthe class that you assign the integer value 1, whereas the other class is assigned the value 0.\\nOutput Layer Configuration : One node with a sigmoid activation unit.\\nLoss Function : Cross -Entropy, also referred to as Logarithmic loss.Neural Network and Deep Learning\\nWhich Loss Function to Use?\\n3. Multi -Class Classification Problem\\nA problem where you classify an example as belonging to one of more  than two \\nclasses.\\nThe problem is framed as predicting the likelihood of an example belonging to \\neach class.\\nOutput Layer Configuration : One node for each class using the softmax  activation \\nfunction.\\nLoss Function : Cross -Entropy, also referred to as Logarithmic loss.Neural Network and Deep Learning\\nRelationship between problems, loss and output activation function\\nTOPICS INDEEP LEARNING\\nNeed for Batch Normalization, Bias  & Variance \\nTradeoff, Regularization, OptimizersTOPICS INDEEP LEARNING\\nHyperparameter Tuning\\n1.Learning rate.\\n2.Momentum beta.\\n3.Mini -batch size.\\n4.No. of hidden units.\\n5.No. of layers.\\n6.Learning rate decay.  7.Regularization lambda.  8.Activation functions.\\n9.Adam beta 1 & beta 2.  10.Dropout rate\\n•Its hard to decide which hyperparameter is the most important in a problem. It \\ndepends a  lot on your problem.\\n•Try random valuesTOPICS INDEEP LEARNING\\nBatch Normalization\\nWhy normalization?\\n1.Ifthe features are ondifferent scale\\n1,1000 and 0,1weights will end up\\ntaking different values .\\n2.More steps may be needed to reach  \\noptimal value and the learning can be  \\nslow.\\n3.Shape of the normalized bowl will be  \\nspherical and symmetrical making  \\neasier to faster to optimize.\\nSubtract meanNormalize varianceTOPICS INDEEP LEARNING\\nBatch Normalization -Summary\\nIdea behind Batch Normalization\\n●It is well known that the inputs to a neural network should be normalized\\nbefore passing them to the network.\\n○This usually involves transforming the inputs to have a mean of zero and unit  \\nvariances.\\n○The inputs should also be decorrelated. All of this is done in order to aid \\nlearning.\\n●So if normalizing the inputs to the input layer improves learning, then normalizing  the\\ninputs to the hidden layers must also improve learning\\nNormalizing thehidden layer inputs isbasically called Batch Normalization .It\\nhelps infaster convergence andlearning andisvery frequently used technique totrain\\nmodels faster .TOPICS INDEEP LEARNING\\nBatch Normalization\\nWhile updating weights in a multilayered Neural network, we update a given layers  \\nthe assumption that the weights of the prior layers have a given distribution .weights under\\nThis distribution is likely changed after the weights of the prior layer are updated. The authors of the  \\npaper introducing batch normalization refer to change in the distribution of inputs during training as  \\n“internal covariate shift.” We define Internal Covariate Shift as the change in the distribution of network  \\nactivations due to the change in network parameters during training.\\nTherefore, we normalize the outputs every layer, intuitively, we are forcing them to have  uniform \\ndistribution throughout training.\\n●Standardizing theactivations oftheprior layer means that assumptions thesubsequent layer\\nmakes about thespread anddistribution ofinputs during theweight update willnotchange, atleast\\nnotdramatically .\\n●This has the effect of stabilizing and speeding -up the training process of deep neural network.TOPICS INDEEP LEARNING\\nBatch Normalization\\nAdvantages ofBatch Normalization\\n●Networks train faster\\n●Allows forhigher learning rates\\n●Makes weights easier to initialise\\n●Provides some regularizationTOPICS INDEEP LEARNING\\nBias andVariance: Introduction\\n•Bias /Variance techniques areEasy to learn, but difficult tomaster .\\n•Sohere theexplanation ofBias /Variance:\\n•Ifyour model isunderfitting (logistic regression of non linear data) ithasa \"high bias\"\\n•Ifyour model isoverfitting then ithasa\"high variance\"\\n•Your model willbealright ifyou balance theBias /Variance\\nhttps://github.com/ashishpatel26/Andrew -NG-Notes/blob/master/andrewng -p-2-improving -deep -learning -network.md#bias --variance\\nTOPICS INDEEP LEARNING\\nBias andVariance: Example\\nhttps://github.com/ashishpatel 26/Andrew -NG-Notes/blob/master/andrewng -p-2-improving -deep -learning -network.md#bias --varianceTraining error 1% 12% 12% 0.5%\\nDev error/  \\nTest error10% 13% 24% 1%\\nHigh Variance High bias High bias and  \\nHigh VarianceLow Bias  Low \\nVariance\\nOver fitting Underfitting Underfitting Best/ Good fitTOPICS INDEEP LEARNING\\nToreduce High Bias\\nhttps://github.com/ashishpatel26/Andrew -NG-Notes/blob/master/andrewng -p-2-improving -deep -learning -network.md#bias --variance1Increase the model size(Try tomake\\nyour NN bigger (size ofhidden units,\\nnumber oflayers)It allows to fit the training set better. If you find this  \\nincreases variance, then use regularization, which  \\nwill usually eliminate variance\\n2Modify input features based on  insight \\nfrom error analysisCreate additional features that help the algorithm  \\neliminate a particular category of errors. These new  \\nfeatures could help with both bias and variance\\n3Reduce or eliminate regularization(  \\nL1,L2,drop out)Reduces avoidable bias, but increases variance\\n4Try to run it longer Might reduce the bias\\n5Modify model architecture This can affect both bias and varianceTOPICS INDEEP LEARNING\\nBias and Variance Trade Off\\n●There is inverse relationship between bias\\nand variance in  machine learning.\\n●Increasing the bias will decrease the variance.\\n●Increasing the variance will decrease the bias.\\n●If our model is too simple and has very few parameters then it\\nmay have high bias and lowvariance .Ontheother hand ifour\\nmodel haslarge number ofparameters then it’sgoing tohave high\\nvariance andlowbias.\\n●We need to find the right/good balance without overfitting and  \\nunderfitting the data.\\n●This tradeoff in complexity is why there is a tradeoff between bias  \\nand variance. An algorithm can ’t be more complex and less  \\ncomplex at the same time.\\nTOPICS INDEEP LEARNING\\nToreduce High Variance\\nReference: https://towardsdatascience.com/regularization -in-machine -learning -76441ddcf99a1Add more training data Simplest and most reliable way to address variance,  \\nso long as you have access to significantly more  \\ndata and enough computational power to process  \\nthe data\\n2Add regularization(L2, L1  \\nregularization, dropout, data  \\naugumentation)This reduces variance, but increases bias\\n3Add early stopping( stop gradient  \\ndescent early based on dev set error)reduces variance, but increases bias\\n4Modify model architecture This can affect both bias and varianceTOPICS INDEEP LEARNING\\nRegularization\\nReference: https://towardsdatascience.com/regularization -in-machine -learning -76441ddcf99aHow doyouprevent overfitting?\\nRegularizationTOPICS INDEEP LEARNING\\nWhy Regularization?\\n●One ofthemajor aspects oftraining your machine\\nlearning model isavoiding overfitting .The model will\\nhave alowaccuracy ifitisoverfitting .\\n●This happens because your model istrying toohard to\\ncapture thenoise inyour training dataset .Bynoise we\\nmean thedata points thatdon’t really represent the\\ntrue properties ofyour data, but random chance .\\nLearning such data points, makes your model more\\nflexible, attheriskofoverfitting .\\n●The concept ofbalancing bias and variance ,is\\nhelpful inunderstanding the phenomenon of\\noverfitting .\\nReference: https://towardsdatascience.com/regularization -in-machine -learning -76441ddcf99aTOPICS INDEEP LEARNING\\nRegularization\\nRegularization\\n●This is a  \\nregularizes orshrinks the coefficientform ofregression ,that constrains /\\nestimates\\ntowards zero.\\n●Inother words, this technique discourages\\nlearning amore complex orflexible model ,soas\\ntoavoid therisk ofoverfitting .\\nReference: https://towardsdatascience.com/regularization -in-machine -learning -76441ddcf99aTOPICS INDEEP LEARNING\\nRegularization\\n●Regularization isatechnique which makes slight\\nmodifications tothelearning algorithm such that the\\nmodel generalizes better .This inturn improves the\\nmodel’s performance ontheunseen data aswell.\\n●Regularization isaprocess ofintroducing additional\\ninformation inorder toprevent overfitting .\\nReference: https://towardsdatascience.com/regularization -in-machine -learning -76441ddcf99aTOPICS INDEEP LEARNING\\nRegularization\\nTypes of \\nRegularization\\n●L2 and L1 \\nregularization\\n●Dropout\\n●Data Augmentation\\n●Early stoppingType ofRegularizationTOPIC SINDEEPLEARNI NG\\n21L1andL2RegularizationTOPICS INDEEP LEARNING\\nL2andL1regularization\\nWhat ’s L1 and L 2?\\nL1 and L2 regularisation owes its name to L1and L 2 norm\\nof a vector w respectively.\\nHere ’s a primer on norms:\\n1-norm (also known as L 1 norm)\\n2-norm (also known as L2 norm or Euclidean norm)\\nTOPICS INDEEP LEARNING\\nL1andL2Regularization\\nAlinear regression model that implements L1norm forregularisation iscalled lasso\\nregression ,and one that implements (squared) L2norm forregularisation iscalled\\nridge regression .\\nToimplement these two, note thatthelinear regression model stays thesame :\\n1.Loss function with no regularisation\\n2.Loss function with L1 regularisation\\n3.Lossfunction withL2regularisat ion\\nTOPICS INDEEP LEARNING\\nWhat does aRegularization achieve?\\nWhat does aRegularization achieve?\\nAstandard least squares model tends tohave some variance init,i.e.thismodel\\nwon’t generalize wellforadata setdifferent than itstraining data.\\nRegularization, significantly reduces the variance ofthe model, without\\nsubstantial increase initsbias.\\nSothetuning parameter λ,used intheregularization techniques, controls theimpact\\nonbias andvariance .\\nAsthevalue ofλrises, itreduces thevalue ofcoefficients and thus reducing the\\nvariance .Tillapoint, this increase inλisbeneficial asitisonly reducing the\\nvariance(hence avoiding overfitting), without loosing any important properties\\ninthedata .\\nButafter certain value, themodel starts loosing important properties, giving riseto\\nbias inthemodel andthus underfitting .Therefore, thevalue ofλshould becarefully\\nselected .\\nReference: https://towardsdatascience.com/regularization -in-machine -learning -76441ddcf99aTOPICS INDEEP LEARNING\\nConclusion about L1andL2Regularization\\nInorder toprevent overfitting ,regularization ismost -approaches\\nmathematical technique, itachieves this bypenalizing thecomplex ML\\nmodels viaadding regularization terms totheloss function/cost function of\\nthemodel .\\nL1regularization gives output inbinary weights from 0to1forthemodel’s\\nfeatures and isadopted fordecreasing thenumber offeatures inahuge\\ndimensional dataset .\\nL2regularization disperse theerror terms inalltheweights thatleads tomore\\naccurate customized final models .\\nReference: https:// www.analyticssteps.com/blogs/l2 -and-l1-regularization -machine -\\nlearningType ofRegularizationTOPIC SINDEEPLEARNI NG\\n33DropoutSo what does dropout do? At every iteration, it randomly selects some nodes and  \\nremoves them along with all of their incoming and outgoing connections as \\nshown  in right figure\\nDropout\\nThis istheoneofthemost interesting types ofregularization techniques .Italso\\nproduces very good results and isconsequently the most frequently used\\nregularization technique inthefield ofdeep learning .\\nTounderstand dropout, let’s sayourneural network structure isakin totheone\\nshown belowTOPICS INDEEP LEARNINGTOPICS INDEEP LEARNING\\nWhat isDropout?\\n•Dropout means leaving some units\\n•Temporarily remove a node and both its incoming and  outgoing \\nconnections.\\n•This thins the neural network\\nNote: Given a total of “n” nodes what are the total number of \\nthinned  networks that can be formed?\\n2^nTOPICS INDEEP LEARNING\\nWhy dropout regularization?\\n•Train several neuralnetwo rks having\\ndifferent architectures\\n•Train multiple instances of\\nthe same  network using different \\ntraining samples.\\n“ Expensive options”\\nBoth at training as well as at \\ntesting  levels\\nHence solution is dropoutTOPICS INDEEP LEARNING\\nDropout Training\\n•Dropout essentially applies a masking noise to the hidden units\\n•Prevents hidden units from coadapting\\n•Essentially a hidden unit cannot rely too much on other units as  they \\nmay get dropped out any time\\n•Each hidden unit has to learn to be more robust to these random  \\ndropouts\\nTOPICS INDEEP LEARNING\\nDropout Training\\nType ofRegularizationTOPIC SINDEEPLEARNI NG\\n42Data Augmentation\\n1.More training data is one more solution for over fitting\\n2.As getting additional data may be expensive and may not be possible\\n3.Flipping of all images can be one of the ways to \\nincrease the data set size.\\n4.Randomly zooming in and zooming out can be another way.\\n5.Distorting some of the images based on your application may be an another\\nway  to increase the data set size.\\nType ofRegularizationTOPIC SINDEEPLEARNI NG\\n42Data Augmentation\\nType ofRegularizationTOPIC SINDEEPLEARNI NG\\n42Data Augmentation\\nPopular Augmentation Techniques:\\n1.Flip\\n2.Rotation\\n3.Scale\\n4.Crop\\n5.Translation\\n6.Gaussian Noise\\n7.Conditional GAN s (Advanced Augmentation Technique)\\nConditional GANs can transform an image from one  domain \\nto an image to another domain.\\nAn example of conditional GANs used to transform  \\nphotographs of summer sceneries to winter  sceneries.TOPICS INDEEP LEARNING\\nData Augmentation\\nReference:https://nanone ts.com/blog/data-augmentation-how-to-use-deep-learning-when-you-have-limited-data-part-2/Type ofRegularizationTOPIC SINDEEPLEARNI NG\\n50Early StoppingTOPICS INDEEP LEARNING\\nEarly Stopping\\nEarly stopping isakind ofcross -validation strategy where wekeep onepartofthe\\ntraining setasthevalidation set.When wesee that theperformance onthe\\nvalidation set(ortestset)isgetting worse, weimmediately stop thetraining onthe\\nmodel .This isknown asearly stopping .\\nThe above image, we will stop training at the dotted line since after that our \\nmodel  will start overfitting on the training data.\\nTOPICS INDEEP LEARNING\\nHow long totrain aneural network?\\n•Too little training will mean that the model will underfit the train and the test  \\nsets.\\n•Too much training will mean that the model will overfit the training dataset  \\nand have poor performance on the test set.\\n•A compromise is to train on the training dataset but to stop training at the\\npoint when performance on a validation dataset starts to degrade .\\n•This simple, effective, and widely used approach to\\ntraining neural  networks is called early stopping .TOPICS INDEEP LEARNING\\nEarly Stopping\\n•Track the validation error\\n•Have a patience parameter p\\n•Ifyouareatstep kandthere was noimprovement\\ninvalidation error intheprevious psteps then stop\\ntraining andreturn themodel stored atstep k−p\\n•Basically, stop thetraining early before itdrives the\\ntraining error to0andblows upthevalidation error\\nIts advantage is that you don\\'t need to search a hyperparameter like in other regularization  \\napproaches (like lambda in L2 regularization).TOPICS INDEEP LEARNING\\nSummary -Why somuch care?\\n•Deep Neural networks are highly complex \\nmodels.\\n•Many parameters, many non -linearities.\\n•It is easy for them to overfit and drive training  \\nerror to 0. Hence we need some form of  \\nregularization.TOPICS INDEEP LEARNING\\nOptimizersTOPICS INDEEP LEARNING\\nOptimizers\\n●Optimizers are algorithm that is used to tweak the parameters such that we reach the\\nminimum of the loss function.\\n●How should one change the weights and biases is defined by the optimizers we use.\\n●Optimizers have been undergoing constant evolution, adding techniques that makes  \\nthem perform faster and more efficientlyTOPICS INDEEP LEARNING\\nOptimizers\\nDifferent types of Optimizers used frequently:\\n1.Gradient Descent\\n2.Stochastic Gradient Descent\\n3.Mini-Batch Gradient Descent\\n4.SGD with Momentum\\n5.Adagrad\\n6.RMS -Prop\\n7.Adam ( Adaptive Moment Estimation -RMS prop and momentum together)TOPICS INDEEP LEARNING\\nOptimizers -Gradient Descent\\nGradient Descent is something we have learnt before.\\n●We find the gradient of the loss function with respect to the weights and biases\\n●And move in small steps towards the opposite direction of the gradient (Gradient  \\npoints to ascent, we want direction of descent to reach the minimum)\\n●Gradient Descent iteratively reduces a loss function by\\nmoving in the direction\\nopposite to that of steepest ascent .\\nTOPICS INDEEP LEARNING\\nOptimizers -Gradient Descent\\nAdvantages\\n●Easy to implement\\n●Good results most of the times\\nDisadvantages\\n●Can get stuck in local minimas\\n●Because this method calculates the gradient for the entire data set in one update, the\\ncalculation is very slow\\n●It requires large memory and it is computationally expensiveTOPICS INDEEP LEARNING\\nOptimizers -Stochastic Gradient Descent\\nVanilla Gradient descent, findthegradient fortheentire dataset, and theupdates the\\nparameters, thismethod isslow andhence theconvergence isslow.Further thismay\\nnotbesuitable forhuge datasets .\\nStochastic GDisavariation ofVanilla Gradient descent were, thegradient isupdated\\nevery data item, instead ofafter going through entire dataset .\\n●Asthemodel parameters arefrequently updated parameters have high variance and\\nfluctuations inlossfunctions atdifferent intensities\\n●SGD uses ahigher number ofiterations toreach thelocal minima .Due toanincrease\\ninthenumber ofiterations, theoverall computation time increases .But even after\\nincreasing thenumber ofiterations, thecomputation cost isstillless than thatofthe\\nvanilla gradient descent optimizer .\\n●Ifthedata isenormous and computational time isanessential factor, stochastic\\ngradient descent should bepreferred over batch gradient descent algorithmTOPICS INDEEP LEARNING\\nOptimizers -Stochastic Gradient Descent\\nAdvantages\\n●Frequent Updation of parameters\\n●Less computationally expensive than Vanilla Gradient Descent\\n●Allows the use of large data sets as it has to update only one example at a time\\n●Uses less memory\\nDisadvantages\\n●The frequent can also result in noisy gradients which may cause the error to  \\nincrease instead of decreasing it.\\n●High Variance.\\nupdates are ●Frequent\\ncomputationally expensive\\n●Stillprone tolocal minima\\nTOPICS INDEEP LEARNING\\nOptimizers -Mini Batch Gradient Descent\\nMini Batch Gradient Descent combines the ideas of Vanilla GD and Stochastic GD .\\nWe shuffle the data, and choose a mini batch of K items, we then calculate gradients for\\nthis mini batch, and update the gradients.\\nThis avoids theproblem ofcomputing gradients fortheentire dataset and also the\\nproblem offrequent updation which causes noisy gradients .Byfinding amiddle ground\\nitisfaster than both thevariants .\\n●Asthealgorithm uses batching, allthetraining data need notbeloaded inthe\\nmemory, thus making theprocess more efficient toimplement\\n●Moreover, thecost function inmini-batch gradient descent isnoisier than thebatch\\ngradient descent algorithm butsmoother than that ofthestochastic gradient descent\\nalgorithm .\\n●Because of this, mini -batch gradient descent is ideal and provides a good balance\\nbetween speed and accuracy.TOPICS INDEEP LEARNING\\nOptimizers -Mini Batch Gradient Descent\\nTOPICS INDEEP LEARNING\\nOptimizers -Mini Batch Gradient Descent\\n•Size of the mini batch needs to be chosen carefully.\\n•If mini batch size =m, then it is batch gradient descent.\\n•If mini batch size=1, then all individual samples are mini batch them selves.  Then \\ntraining becomes very slow.\\n•The size of the mini batch need not be too small or too big.\\nTo take advantage of the CPU/GPU Memory mini batch size may be in the context  of \\nthe available RAM.\\n•Generally the mini batch size is taken as power of 2\\n•If the training set is less than 2000, then there is no need of using mini -batch.TOPICS INDEEP LEARNING\\nSummary\\n•(mini batch size=m)==> Batch gradient descent\\n•(mini batch size=1)==> Stochastic gradient descent (SGD)\\n•(mini batch size=between 1and m)==> Mini -batch gradient descent\\n•Mini -batch sizeis a hyperparameterTOPICS INDEEP LEARNING\\nOptimizers -Mini Batch Gradient Descent\\nAdvantages:\\n●Faster and more efficient than previous variants of GD\\n●More efficient gradient calculation\\n●Requires less amount of memory (Loads only one batch per updation)\\nDisadvantages:\\n●Still prone to local minima\\n●It introduces another hyperparameter K (the batch size)\\nTOPICS INDEEP LEARNING\\nChallenges with all variants of GD\\n●Choosing an optimum value of the learning rate.\\n●If the learning rate is too small than gradient descent may take ages to converge.\\n●Having a constant learning rate for all the parameters.\\n●There may be some parameters which we may not want to change at the same rate.\\n●May get trapped at local minima.TOPICS INDEEP LEARNING\\nLearning Rate DecayTOPICS INDEEP LEARNING\\nLearning Rate Decay\\n•Slowly reduce learning rate.\\n•One technique equations is\\nlearning_rate = (1 / (1 + decay_rate * epoch_num)) * learning_rate\\n•Other learning rate decay methods (continuous):\\n•learning_rate = (0.95 ^ epoch_num) * learning_rate_0\\n•learning_rate = (k / sqrt(epoch_num)) * learning_rate_0\\n•Some people perform learning rate decay discretely -repeatedly decrease after some number of  \\nepochs.\\n•Some people are making changes to the learning rate manually.\\n•decay_rate is another hyperparameter.\\n•learning rate decay has less priority.TOPICS INDEEP LEARNING\\nFine -Tuning Hyper parametersNeural Network and Deep Learning\\nx1\\nx2\\n……\\nx…………\\n……\\n…………y1\\ny2\\ny100.1\\n0.7\\n0.2\\numvalue16x16=256\\nSetthenetwork parameters 𝜃such that ……\\numvalueInp\\nInput: y1hasthemaxim\\nut: y2hasthemaxim\\nis1\\nis2\\nis0\\nHow to lettheneural\\nnetwork achieve thisFine -Tuning Hyper parameters\\n𝜃=𝑊1, 𝑏1, 𝑊2, 𝑏2, ⋯ 𝑊𝐿, 𝑏𝐿\\n……Neural Network and Deep Learning\\nHow to try Hyperparameters?\\n.\\nNeural Network and Deep Learning\\nDeep Neural Network: Parameters vsHyperparameters\\nParameters:\\n𝑊1, 𝑏1, 𝑊2, 𝑏2, ⋯ 𝑊𝐿, 𝑏𝐿\\nHyperparameters:\\nLearning rate 𝑎 in gradient descent  Number of iterations in gradient descent  Number of layers \\nin a Neural Network\\nNumber of neurons per layer in a Neural Network\\nActivations Functions  Mini -batch size\\nRegularizations parameters\\n•You have to try values yourself of hyper parameters.Neural Network and Deep Learning\\nTrain /Dev /Test sets\\nHyperparameters tuning is a highly iterative process, where you  \\nstart with an idea, i.e. start with a certain number of  \\nhidden layers, certain learning rate, etc.\\ntry the idea by implementing it  experiment how well the idea \\nhas worked  refine the idea and iterate this process\\nNow how do we identify whether the idea is working? This is\\nwhere the train / dev / test sets come into play.\\nTraining Set\\nDev Set\\nTestSetWetrain themodel onthetraining data.\\nAfter training themodel, wecheck how well itperforms onthedev set.\\nWhen we have a final model, we evaluate it on the test set in order to get  an \\nunbiased estimate of how well our algorithm is doing.DataTrain/Dev/Testsets\\nTraining Set  \\n(60%)\\nDev Set  \\n(20%)\\nTest Set  \\n(20%)Training Set  \\n(70%)\\nTest Set  \\n(30%)Training Set  \\n(98%)\\nDev Set (1%)\\nTest Set (1%)\\nPreviously,whe nwe\\ndatasets, most oftenhad small\\nthe\\ndistribution ofdifferent sets wasAsthe availability ofdata has increased\\ninrecent years, wecan use ahuge slice\\nofitfortraining themodelDataNeural Network and Deep LearningNeural Network and Deep Learning\\nFine -Tuning Hyper parameters\\n•We need to tune our hyperparameters to get the best out of them.\\n•Hyperparameters importance are (as for Andrew Ng):\\n•Learning rate.\\n•Momentum beta.\\n•Mini -batch size.\\n•No. of hidden units.\\n•No. of layers.\\n•Learning rate decay.\\n•Regularization lambda.\\n•Activation functions.\\n•Adam beta1 & beta2.\\n•Its hard to decide which hyperparameter is the most important in a problem. It depends a lot on your problem.\\n•One of the ways to tune is to sample a grid with N hyperparameter settings and then try all settings combinations on  \\nyour problem.\\n•Try random values: don\\'t use a grid.Neural Network and Deep Learning\\nFine -Tuning Hyper parameters -Grid Search\\nTaken from the imperative command \"Just try everything!\" comes Grid Search –a naive  \\napproach of simply trying every possible configuration.\\nHere\\'s the workflow:\\n•Define a grid on n dimensions, where each of these maps for an hyperparameter. e.g. n =\\n(learning_rate, dropout_rate, batch_size)\\n•For each dimension, define the range of possible values:  e.g. batch_size = [4, 8, 16, 32, \\n64, 128, 256]\\n•Search for all the possible configurations and wait for the results to establish the best  \\none:\\ne.g. C1 = (0.1, 0.3, 4) -> acc = 92%,\\nC2 = (0.1, 0.35, 4) -> acc = 92.3%, etc...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "FTLhPa3NZAq5"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sewlsp17bIfZ",
        "outputId": "cfd32e28-f332-4f17-94c9-b6a4e92c9363"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "106"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "xW77s2-5bKOR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doccument_search = FAISS.from_texts(texts, embeddings)"
      ],
      "metadata": {
        "id": "ZmetWuyvbTJw"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doccument_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ2klfSGbddw",
        "outputId": "562a9a56-cd96-4ff6-826f-e3205e670d0b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x7c44dad3fb50>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "QrZ9MxyEbldB"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(OpenAI(), chain_type = \"stuff\")"
      ],
      "metadata": {
        "id": "xxVwNj4kb1-o"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"List the types of activation functions in deep learning\"\n",
        "docs = doccument_search.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "cC4uR6LOb-Wh",
        "outputId": "c3261179-f8f0-4be4-e8ea-4b6c4f687d99"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The types of activation functions in deep learning are binary step function, linear function, and non-linear activation functions such as sigmoid, softmax, ReLU, Leaky ReLU, and tanh.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AHclDFswg7E6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}